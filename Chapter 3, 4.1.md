## ex1


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

hello = tf.constant('Hello. TensorFlow!')
print(hello)
```

    WARNING:tensorflow:From c:\users\rlawl\appdata\local\programs\python\python37\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
    Instructions for updating:
    non-resource variables are not supported in the long term
    Tensor("Const:0", shape=(), dtype=string)
    


```python
sess = tf.Session()

print(sess.run(hello).decode())
```

    Hello. TensorFlow!
    


```python
a=tf.constant(10)
b=tf.constant(32)
c=tf.add(a,b)
```


```python
print(sess.run(c))
```

    42
    

## ex2


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

X = tf.placeholder(tf.float32, [None, 3])
print(X)

x_data = [[1,2,3], [4,5,6]]

W = tf.Variable(tf.random_normal([3,2]))
b = tf.Variable(tf.random_normal([2,1]))

expr = tf.matmul(X,W) + b

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print("=== x_data ===")
print(x_data)
print("=== W ===")
print(sess.run(W))
print("=== b ===")
print(sess.run(b))
print("=== expr ===")

print(sess.run(expr, feed_dict={X:x_data}))

sess.close()
```

    Tensor("Placeholder_1:0", shape=(?, 3), dtype=float32)
    === x_data ===
    [[1, 2, 3], [4, 5, 6]]
    === W ===
    [[-1.1776156   0.93957716]
     [-0.47281367 -1.8831422 ]
     [-0.6935132  -0.56035024]]
    === b ===
    [[-0.07776187]
     [ 1.2804743 ]]
    === expr ===
    [[-4.281544  -4.58552  ]
     [-9.955135  -7.7390294]]
    

## ex3


```python
x_data = [1,2,3]
y_data = [1,2,3]

W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

X = tf.placeholder(tf.float32, name="X")
Y = tf.placeholder(tf.float32, name="Y")

hypothesis = W * X + b

cost = tf.reduce_mean(tf.square(hypothesis - Y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = optimizer.minimize(cost)
```


```python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for step in range(100):
        _,cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})
    
        print(step, cost_val, sess.run(W), sess.run(b))
        
    print("\n=== Test ===")
    print("X: 5, Y:", sess.run(hypothesis, feed_dict={X: 5}))
    print("X: 2.5, Y:", sess.run(hypothesis, feed_dict={X: 2.5}))
```

    0 19.226137 [1.0068722] [0.48939943]
    1 0.25318515 [0.80469835] [0.38877067]
    2 0.02543188 [0.8314717] [0.3891372]
    3 0.021646922 [0.83310986] [0.3787211]
    4 0.020587897 [0.8373856] [0.36973295]
    5 0.019609591 [0.8412659] [0.36083212]
    6 0.018678097 [0.84508485] [0.35215932]
    7 0.017790893 [0.8488086] [0.34369352]
    8 0.016945807 [0.85244316] [0.33543137]
    9 0.016140876 [0.85599035] [0.32736784]
    10 0.015374172 [0.85945225] [0.31949812]
    11 0.01464387 [0.86283094] [0.31181762]
    12 0.013948269 [0.8661283] [0.3043217]
    13 0.013285725 [0.86934656] [0.29700604]
    14 0.012654644 [0.87248737] [0.2898662]
    15 0.012053532 [0.87555265] [0.282898]
    16 0.011480983 [0.87854433] [0.27609736]
    17 0.010935628 [0.88146406] [0.26946017]
    18 0.0104161715 [0.8843135] [0.26298252]
    19 0.009921405 [0.88709456] [0.2566606]
    20 0.0094501255 [0.8898087] [0.25049067]
    21 0.009001233 [0.89245766] [0.24446906]
    22 0.00857368 [0.8950429] [0.23859218]
    23 0.00816642 [0.897566] [0.23285659]
    24 0.0077785156 [0.90002847] [0.22725886]
    25 0.0074090273 [0.90243167] [0.2217957]
    26 0.007057091 [0.90477717] [0.21646388]
    27 0.006721873 [0.9070663] [0.21126023]
    28 0.006402578 [0.9093003] [0.20618166]
    29 0.006098446 [0.91148067] [0.20122519]
    30 0.005808765 [0.9136086] [0.19638787]
    31 0.0055328473 [0.9156854] [0.19166686]
    32 0.005270034 [0.9177123] [0.18705933]
    33 0.0050196997 [0.9196904] [0.18256254]
    34 0.004781267 [0.92162097] [0.17817388]
    35 0.0045541516 [0.9235052] [0.17389072]
    36 0.004337828 [0.92534405] [0.1697105]
    37 0.004131776 [0.92713875] [0.16563079]
    38 0.0039355126 [0.9288903] [0.16164914]
    39 0.0037485708 [0.9305997] [0.15776318]
    40 0.0035705166 [0.932268] [0.15397066]
    41 0.0034009113 [0.9338963] [0.15026931]
    42 0.0032393641 [0.93548536] [0.14665695]
    43 0.0030854952 [0.9370362] [0.1431314]
    44 0.002938931 [0.9385499] [0.13969064]
    45 0.0027993328 [0.9400271] [0.13633257]
    46 0.002666366 [0.94146883] [0.13305523]
    47 0.0025397004 [0.9428758] [0.12985665]
    48 0.0024190708 [0.9442491] [0.126735]
    49 0.002304164 [0.9455893] [0.12368836]
    50 0.0021947068 [0.94689727] [0.12071496]
    51 0.0020904574 [0.9481739] [0.11781307]
    52 0.0019911642 [0.9494197] [0.1149809]
    53 0.0018965775 [0.9506356] [0.11221686]
    54 0.0018064906 [0.9518223] [0.10951924]
    55 0.0017206809 [0.95298046] [0.10688648]
    56 0.0016389495 [0.9541108] [0.10431702]
    57 0.0015610988 [0.9552139] [0.1018093]
    58 0.0014869448 [0.9562906] [0.0993619]
    59 0.0014163145 [0.9573413] [0.09697329]
    60 0.0013490353 [0.9583668] [0.09464211]
    61 0.001284955 [0.95936763] [0.09236697]
    62 0.0012239207 [0.96034443] [0.09014654]
    63 0.0011657798 [0.96129763] [0.08797945]
    64 0.0011104029 [0.962228] [0.08586448]
    65 0.0010576613 [0.9631361] [0.08380039]
    66 0.0010074264 [0.9640223] [0.08178589]
    67 0.0009595671 [0.9648871] [0.07981979]
    68 0.00091399014 [0.96573126] [0.07790101]
    69 0.0008705701 [0.96655494] [0.07602828]
    70 0.0008292186 [0.967359] [0.07420065]
    71 0.0007898352 [0.9681437] [0.07241691]
    72 0.000752313 [0.96890944] [0.07067603]
    73 0.0007165784 [0.9696569] [0.06897705]
    74 0.00068254065 [0.9703863] [0.06731889]
    75 0.0006501169 [0.9710982] [0.06570058]
    76 0.00061923766 [0.971793] [0.0641212]
    77 0.000589822 [0.97247106] [0.06257976]
    78 0.0005618059 [0.9731328] [0.06107537]
    79 0.00053512084 [0.9737787] [0.05960718]
    80 0.00050970074 [0.97440904] [0.05817425]
    81 0.0004854918 [0.9750243] [0.05677579]
    82 0.00046242718 [0.9756246] [0.05541093]
    83 0.00044046412 [0.9762106] [0.05407889]
    84 0.00041954304 [0.9767825] [0.05277888]
    85 0.00039961303 [0.97734064] [0.0515101]
    86 0.00038063317 [0.9778853] [0.05027182]
    87 0.00036255093 [0.978417] [0.04906333]
    88 0.00034532836 [0.9789358] [0.04788386]
    89 0.00032892544 [0.9794422] [0.04673278]
    90 0.00031330116 [0.97993636] [0.04560936]
    91 0.00029841883 [0.9804186] [0.04451293]
    92 0.0002842444 [0.98088944] [0.0434429]
    93 0.0002707403 [0.98134875] [0.04239853]
    94 0.00025788337 [0.9817972] [0.04137933]
    95 0.00024563321 [0.9822348] [0.04038459]
    96 0.00023396501 [0.9826618] [0.03941376]
    97 0.00022285152 [0.9830786] [0.03846629]
    98 0.00021226589 [0.9834854] [0.03754159]
    99 0.00020218264 [0.9838824] [0.03663912]
    
    === Test ===
    X: 5, Y: [4.9560513]
    X: 2.5, Y: [2.4963453]
    


```python

```
